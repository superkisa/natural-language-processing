Slides:
[link](https://github.com/girafe-ai/natural-language-processing/blob/master/week04_attention/MSAI_NLP_f21_lect104_Attention_and_self_attention.pdf)

NMT with attention:
* Completed version:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/natural-language-processing/blob/master/week04_attention/practice1_04_seq2seq_nmt__with_attention.ipynb)

* Basic attention intro and bonus assignment:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/natural-language-processing/blob/master/week04_attention/practice1_04_extra_attention_basics_and_tensorboard.ipynb)


Further readings:

* Great explanation of attention and seq2seq translation by Lena Voita: https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn
* Great blog post by Jay Alammar: http://jalammar.github.io/illustrated-transformer/
