Slides:
[link](https://github.com/girafe-ai/natural-language-processing/blob/master/week03_machine_translation/MSAI_NLP_f21_lect103_Machine_Tranlation.pdf)

NMT and tensorboard tutorial:
* Self-practice version:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/natural-language-processing/blob/master/week03_machine_translation/practice1_03_seq2seq_nmt_and_tensorboard.ipynb)

* Solved version:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/natural-language-processing/blob/master/week03_machine_translation/practice1_03_seq2seq_nmt_and_tensorboard__completed.ipynb)


Further readings:

* Great explanation of attention and seq2seq translation by Lena Voita: https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn